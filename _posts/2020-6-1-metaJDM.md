Current graduate student, Kelli Johnson, and former graduate student, [Michael Bixter](https://www.montclair.edu/profilepages/view_profile.php?username=bixterm), have published [an article in *Judgement and Decision Making*](http://www.sjdm.org/journal/18/18308/jdm18308.pdf).  The paper reports a meta-analysis of published associations between preferences about delayed rewards and preferences about risky rewards.  Results suggest an association that is consistently non-zero, but too weak to provide any support for existing theories.  You can read the article itself for more details.  But here, I wish to use this study to highlight a broader issue.

Multiple theories, from multiple disciplines, have proposed that delay and risk influence behavior through a single, common mechanism.  The most intuitive of these (my personal opinion) suggests that delay necessarily implies risk.  Imagine lending someone $100.  Is it more plausible that this money will be repaid if it is due in a week or if it's due in 3 years?  Presumably there are a number of things that could prevent repayment: the borrower could lose it all in Vegas, the borrower could skip town, the stock market could crash, one of you could become ill, incapacitated, or even die.  Presumably each of these become more probable as the delay period is prolonged implying that delay entails risk[1].

According to this account, choices about intertemporal choices (e.g., $50 today or $100 in 2 weeks)  ought to be made using the same mechanism as risky choices (e.g., a guaranteed $50 or a 50%-50% chance of $100 or $0).  How does one evaluate such prediction?  In the studies we meta-analyzed, researchers had participants make a bunch of intertemporal choices and make a bunch of risky choices.  Preferences in each task were quantified and individual differences in each of the tasks two tasks were associated.  In common practice, significant, non-zero associations are taken as evidence supporting the single-process accounts and non-significant associations are taken as evidence against the single-process accounts.  Easy!

The problem, however, is that "significant association between individual differences" isn't actually a prediction made by any of these accounts.  At best, it's an indirect implication that one can draw once a slew of additional assumptions are added into the mix.  For example, even if delay entails risk, one needs to specify exactly how these quantities are related (e.g., Sozou, 1998; Dasgupta & Maskin, 2005).  It is extremely unlikely that any specification would yield a straightfoward, linear relationship which complicates predictions.  Furthermore, different decision makers may hold different beliefs about the relationship between delay and risk, complicating the interpretation of individual differences.  All in all, if you sit down to read the original theoretical proposals and try to work out exactly what constraints they pose regarding the empirically observed associations, you'd likely come up empty.

So why do researchers interpret the associations in such a facile manner?  Part of it is presumably that it's just simpler to deal with "theory predicts association" than it is to engage with the all the nitty-gritty details that need to be confronted when diving into the actual (under-specified) theory.  This is an unfortunate, but all-too-common pattern.  Eiko Fried (2020) argues that researchers often conflate theoretical models and statistical models.  And Tal Yarkoni (2019) has recently wrote the following:

> It is not simply a matter of researchers slightly overselling their conclusions here and there; the fundamental problem is that, on close examination, a huge proportion of the verbal claims made in empirical psychology articles turn out to have very little relationship with the statistical quantities they putatively draw their support from.

That sounds bad!  Though both of these researchers are discussing applications involving complex statistical models (for the most part), I think similar confusion occurs even in cases when the statistical model is trivial.  For example, single-process theories make claims about decision-specific processes and at least insuinuate quantitative rigor.  But researchers often act as if the theory is "risk-delay correlation > 0".

To say that this is lazy thinking is an understatement.  The broader problem, however, is that the literature is littered with empirical findings that, collectively, appear to represent scientific progress but actually mean very little.  As we make clear in [the paper](http://www.sjdm.org/journal/18/18308/jdm18308.pdf), the empirical data is not good **for** the single-process theories typically implicated, but it's also not good evidence **against** these theories.  It's mush.

So yeah.  I think the take-home messages are:

1. Be *thoughtful* when connecting theories to empirical predictions.
2. Assume that these connections are more complicated than you'd hope (Jolly & Chang, 2019).
3. Distinguish between the statistical model used to evaluate predictions from the theory that predicted those predictions.
 

---

[1] We once submitted a manuscript (Bixter & Luhmann, 2015) and one of the reviewers suggested that we simply explain to participants that the delayed rewards (e.g., $100 to be delivered in a month) were to be treated as *certain*; that they were guaranteed to be received.  But how could such a guarantee be given?  And why would a participant accept such a guarantee?


[Bixter, M. T., & Luhmann, C. C. (2015). Evidence for implicit risk: Delay facilitates the processing of uncertainty. Journal of Behavioral Decision Making, 28(4), 347-359.](https://doi.org/10.1002/bdm.1853)

Dasgupta, P., & Maskin, E. (2005). Uncertainty and hyperbolic discounting. American Economic Review, 95(4), 1290-1299.

[Fried, E. I. (2020, February 7). Lack of theory building and testing impedes progress in the factor and network literature.](https://doi.org/10.31234/osf.io/zg84s)

[Jolly, E., & Chang, L. J. (2019). The Flatland Fallacy: Moving Beyond Lowâ€“Dimensional Thinking. Topics in cognitive science, 11(2), 433-454.](https://doi.org/10.1111/tops.12404)

Sozou, P. D. (1998). On hyperbolic discounting and uncertain hazard rates. Proceedings of the Royal Society of London. Series B: Biological Sciences, 265(1409), 2015-2020.

[Yarkoni, T. (2019, November 22). The Generalizability Crisis.](https://doi.org/10.31234/osf.io/jqw35)
